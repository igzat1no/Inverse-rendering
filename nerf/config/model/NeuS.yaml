optimizer:
  name: Adam
  params:
    lr: 0.01
    betas: [0.9, 0.99]
    eps: 1.e-15

warmup_steps: 500
lr_decay: 0.1
remaining_steps: ${eval:${iteration} - ${warmup_steps}}
lr_factor: ${eval:1.0 / ${remaining_steps}}

scheduler:
  name: SequentialLR
  schedulers:
    - name: LinearLR
      params:
        start_factor: 0.01
        end_factor: 1.0
        total_iters: ${warmup_steps}
    - name: ExponentialLR
      params:
        gamma: ${eval:${lr_decay} ** ${lr_factor}}
  params:
    milestones:
      - ${warmup_steps}

model:
  radius: 1.5
  num_samples_per_ray: 1024
  occ_grid_reso: 128
  cos_anneal_end: 20000
  use_mask: True
  randomized: True
  grid_prune_occ_thre: 0.001

  variance:
    init_val: 0.3
    modulate: False

  geometry:
    name: volume-sdf
    radius: ${..radius}
    feature_dim: 13
    grad_type: analytic
    isosurface:
      method: mc
      resolution: 512
      chunk: 2097152
      threshold: 0.
    xyz_encoding_config:
      otype: HashGrid
      n_levels: 16
      n_features_per_level: 2
      log2_hashmap_size: 19
      base_resolution: 16
      per_level_scale: 1.447269237440378
      include_xyz: True
    mlp_network_config:
      otype: VanillaMLP
      activation: ReLU
      output_activation: none
      n_neurons: 32
      n_hidden_layers: 1
      sphere_init: True
      sphere_init_radius: 0.5
      weight_norm: True

  texture:
    name: volume-radiance
    input_feature_dim: ${add:${..geometry.feature_dim},3} # surface normal as additional input
    dir_encoding_config:
      otype: SphericalHarmonics
      degree: 4
    mlp_network_config:
      otype: FullyFusedMLP
      activation: ReLU
      output_activation: none
      n_neurons: 64
      n_hidden_layers: 2

  loss:
    lambda_rgb_mse: 10.
    lambda_rgb_l1: 0.
    lambda_mask: 0.1
    lambda_eikonal: 0.1
    lambda_sparsity: 0.01
    sparsity_scale: 1.
    lambda_distortion: 0.
    lambda_opaque: 0.
